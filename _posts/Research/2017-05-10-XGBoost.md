---
layout: post
title: Introduction to Boosted Trees
description: ""
category: 研究
tags: [Note]
---
## Introduction to Boosted Trees

XGBoost is short for “Extreme Gradient Boosting”, where the term “Gradient Boosting” is proposed in the paper ***Greedy Function Approximation: A Gradient Boosting Machine***, by Friedman. XGBoost is based on this original model. This is a tutorial on gradient boosted trees, and most of the content is based on these [slides](http://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf) by the author of xgboost.

The GBM (boosted trees) has been around for really a while, and there are a lot of materials on the topic. This tutorial tries to explain boosted trees in a self-contained and principled way using the elements of supervised learning. We think this explanation is cleaner, more formal, and motivates the variant used in xgboost

## Elements of Supervised Learning

XGBoost is used for supervised learning problems, where we use the training data (with multiple features) **xi** to predict a target variable **yi**. Before we dive into trees, let us start by reviewing the basic elements in supervised learning

## Model and Parameters

The model in supervised learning usually refers to the mathematical structure of how to make the prediction **yi** given **xi**. For example, a common model is a linear model, where the prediction is given by **y^i=∑jθjxij**, a linear combination of weighted input features. The prediction value can have different interpretations, depending on the task, i.e., regression or classification. For example, it can be logistic transformed to get the probability of positive class in logistic regression, and it can also be used as a ranking score when we want to rank the outputs.

The parameters are the undetermined part that we need to learn from data. In linear regression problems, the parameters are the coefficients [Math Processing Error]θ. Usually we will use [Math Processing Error]θ to denote the parameters (there are many parameters in a model, our definition here is sloppy

## Objective Function : Training Loss + Regularization
Based on different understandings of [Math Processing Error]yi we can have different problems, such as regression, classification, ordering, etc. We need to find a way to find the best parameters given the training data. In order to do so, we need to define a so-called objective function, to measure the performance of the model given a certain set of parameters.

A very important fact about objective functions is they must always contain two parts: training loss and regularization.

Obj(Θ)=L(θ)+Ω(Θ)
where [Math Processing Error]L is the training loss function, and [Math Processing Error]Ω is the regularization term. The training loss measures how predictive our model is on training data. For example, a commonly used training loss is mean squared error.

L(θ)=∑i(yi−y^i)2
Another commonly used loss function is logistic loss for logistic regression

L(θ)=∑i[yiln⁡(1+e−y^i)+(1−yi)ln⁡(1+ey^i)]

The regularization term is what people usually forget to add. The regularization term controls the complexity of the model, which helps us to avoid overfitting. This sounds a bit abstract, so let us consider the following problem in the following picture. You are asked to fit visually a step function given the input data points on the upper left corner of the image. Which solution among the three do you think is the best fit? 

![]({{ site.url }}/assets/images/xgboost_1.png)

he correct answer is marked in red. Please consider if this visually seems a reasonable fit to you. The general principle is we want both a simple and predictive model. The tradeoff between the two is also referred as bias-variance tradeoff in machine learning.

Why introduce the general principle?
The elements introduced above form the basic elements of supervised learning, and they are naturally the building blocks of machine learning toolkits. For example, you should be able to describe the differences and commonalities between boosted trees and random forests. Understanding the process in a formalized way also helps us to understand the objective that we are learning and the reason behind the heuristics such as pruning and smoothing.

Tree Ensemble
Now that we have introduced the elements of supervised learning, let us get started with real trees. To begin with, let us first learn about the model of xgboost: tree ensembles. The tree ensemble model is a set of classification and regression trees (CART). Here’s a simple example of a CART that classifies whether someone will like computer games.

![]({{ site.url }}/assets/images/xgboost_2.png)

We classify the members of a family into different leaves, and assign them the score on the corresponding leaf. A CART is a bit different from decision trees, where the leaf only contains decision values. In CART, a real score is associated with each of the leaves, which gives us richer interpretations that go beyond classification. This also makes the unified optimization step easier, as we will see in a later part of this tutorial.

Usually, a single tree is not strong enough to be used in practice. What is actually used is the so-called tree ensemble model, which sums the prediction of multiple trees together.

![]({{ site.url }}/assets/images/xgboost_3.png)

Here is an example of a tree ensemble of two trees. The prediction scores of each individual tree are summed up to get the final score. If you look at the example, an important fact is that the two trees try to complement each other. Mathematically, we can write our model in the form

    \hat{y}_i = \sum_{k=1}^K f_k(x_i), f_k \in \mathcal{F}

\text{obj}(\theta) = \sum_i^n l(y_i, \hat{y}_i) + \sum_{k=1}^K \Omega(f_k)
